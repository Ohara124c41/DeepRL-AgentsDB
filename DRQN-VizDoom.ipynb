{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Q-Network for VizDoom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "\n",
    "from helper2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, False, False, False, False, False, False], [False, True, False, False, False, False, False], [False, False, True, False, False, False, False], [False, False, False, True, False, False, False], [False, False, False, False, True, False, False], [False, False, False, False, False, True, False], [False, False, False, False, False, False, True]]\n"
     ]
    }
   ],
   "source": [
    "from vizdoom import *\n",
    "a_size = 7 # Agent can move Left, Right, or Fire\n",
    "image_size = 84\n",
    "\n",
    "use_other_buffers = False #Wheter we use the depth buffer and label buffer instead of the screen buffer\n",
    "use_RGB = False #Whether we use GRB or black and white\n",
    "if use_other_buffers == True:\n",
    "    image_chls = 2\n",
    "else:\n",
    "    if use_RGB == True:\n",
    "        image_chls = 3\n",
    "    else:\n",
    "        image_chls = 1\n",
    "\n",
    "#The Below code is related to setting up the Doom environment\n",
    "game = DoomGame()\n",
    "game.set_doom_scenario_path(\"deadly_corridor.wad\")  #This corresponds to the simple task we will pose our agent\n",
    "game.load_config(\"deadly_corridor.cfg\")\n",
    "game.set_doom_map(\"map01\")\n",
    "game.set_screen_resolution(ScreenResolution.RES_160X120)\n",
    "\n",
    "if use_RGB == True:\n",
    "    game.set_screen_format(ScreenFormat.RGB8)\n",
    "else:\n",
    "    game.set_screen_format(ScreenFormat.GRAY8)\n",
    "    \n",
    "game.set_render_hud(False)\n",
    "game.set_render_crosshair(False)\n",
    "game.set_render_weapon(True)\n",
    "game.set_render_decals(False)\n",
    "game.set_render_particles(False)\n",
    "\n",
    "#Enable other usefull buffers for test purpose\n",
    "game.set_depth_buffer_enabled(True)\n",
    "game.set_automap_buffer_enabled(True)\n",
    "game.set_labels_buffer_enabled(True)\n",
    "\n",
    "#game.add_available_button(Button.TURN_LEFT)\n",
    "#game.add_available_button(Button.TURN_RIGHT)\n",
    "#game.add_available_button(Button.ATTACK)\n",
    "actions_list = np.identity(a_size,dtype=bool).tolist()\n",
    "print(actions_list)\n",
    "\n",
    "game.add_available_game_variable(GameVariable.AMMO2)\n",
    "game.add_available_game_variable(GameVariable.POSITION_X)\n",
    "game.add_available_game_variable(GameVariable.POSITION_Y)\n",
    "game.set_episode_timeout(300)\n",
    "game.set_episode_start_time(0)\n",
    "game.set_window_visible(False)\n",
    "game.set_sound_enabled(False)\n",
    "#game.set_living_reward(-1)\n",
    "game.set_mode(Mode.PLAYER)\n",
    "game.init()\n",
    "\n",
    "#End Doom set-up\n",
    "\n",
    "env = game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,rnn_cell,myScope,learning_rate):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        \n",
    "            \n",
    "        self.scalarInput =  tf.placeholder(shape=[None,image_size * image_size * image_chls],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,image_size,image_size,image_chls])\n",
    "        self.conv1 = slim.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,\\\n",
    "            kernel_size=[8,8],stride=[4,4],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv1')\n",
    "        self.conv2 = slim.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,\\\n",
    "            kernel_size=[4,4],stride=[2,2],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv2')\n",
    "        self.conv3 = slim.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,\\\n",
    "            kernel_size=[3,3],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv3')\n",
    "        self.conv4 = slim.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,\\\n",
    "            kernel_size=[7,7],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv4')\n",
    "        \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        #We take the output from the final convolutional layer and send it to a recurrent layer.\n",
    "        #The input must be reshaped into [batch x trace x units] for rnn processing, \n",
    "        #and then returned to [batch x units] when sent through the upper levles.\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.conv4),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        #The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "        self.streamA,self.streamV = tf.split(self.rnn,2,1)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,a_size]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        #In order to only propogate accurate gradients through the network, we will mask the first\n",
    "        #half of the losses for each trace as per Lample & Chatlot 2016\n",
    "        self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "        self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "        self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "        self.mask = tf.reshape(self.mask,[-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes allow us to store experies and sample then randomly to train the network.\n",
    "Episode buffer stores experiences for each individal episode.\n",
    "Experience buffer stores entire episodes of experience, and sample() allows us to get training batches needed from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, trace_length = 8, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        self.trace_length = trace_length\n",
    "\n",
    "        #Initialize counters and buffers for prioritixed replay\n",
    "        self.episode_index = -1\n",
    "        self.alpha0 = 0.5 #Start-value of alpha, the prioritized replay probability exponent. Annealing is linear to 0.\n",
    "        self.alpha = self.alpha0\n",
    "        self.exp_prio_tuples = []\n",
    "        \n",
    "    def add(self,episode):\n",
    "        #Compute the sampling priority of this episode in episode replay and update the sum of priorities\n",
    "        \n",
    "        episode = np.reshape(np.array(episode),[len(episode),6])\n",
    "        self.td_error = episode[:, 5]\n",
    "        self.priority = np.absolute(self.td_error) + 1e-18 #proportionnal priority\n",
    "\n",
    "        #Need something to avoid self.exp_prio_tuples to grow infinitly\n",
    "        if len(self.buffer)+1 > self.buffer_size:\n",
    "            cntr = 0\n",
    "            while cntr < self.exp_prio_tuples[0][3] and self.exp_prio_tuples[0][3] == len(self.buffer[0])-self.trace_length :\n",
    "                self.exp_prio_tuples.pop(0)\n",
    "                cntr += 1\n",
    "            \n",
    "            #print('length of poped element = ' + str(len(self.buffer[0]))+ ' , cntr = ' + str(cntr)+' , diff = ' +str(len(self.buffer[0])-cntr))\n",
    "            self.buffer.pop(0)\n",
    "        \n",
    "        #Append episode to the priority replay tuple list\n",
    "        #Every experience in the episode has a tuple of the form:\n",
    "        #(episode_index, experience_index, priority, len(episode))\n",
    "        self.episode_index += 1\n",
    "        for experience_index in range(self.trace_length-1, len(episode)-1):\n",
    "            tup = (self.episode_index, experience_index, float(self.priority[experience_index]), len(episode)-self.trace_length)\n",
    "            self.exp_prio_tuples.append(tup)    \n",
    "        self.buffer.append(episode)\n",
    "        \n",
    "        '''\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(episode)\n",
    "        '''\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        #Ramdomly select a number of episodes egual to batch_size \n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        #Within the selected episodes, randomly select an experience trace of length trace_length\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-self.trace_length)\n",
    "            sampledTraces.append(episode[point:point+self.trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        \n",
    "        return np.reshape(sampledTraces,[batch_size*self.trace_length,6])\n",
    "    \n",
    "    def PRsample(self,batch_size):\n",
    "        #alpha annealing\n",
    "        self.alpha = self.alpha0 - (self.episode_index * self.alpha0/num_episodes)\n",
    "        #Compute the sampling probability distribution\n",
    "\n",
    "        \n",
    "        priorities_poweralpha = np.power([tup[2] for tup in self.exp_prio_tuples],myBuffer.alpha)\n",
    "        sum_priorities_poweralpha = np.sum(priorities_poweralpha)\n",
    "        sampling_probabilities = np.divide(priorities_poweralpha, sum_priorities_poweralpha)\n",
    "        #Sample episodes using the computed distribution\n",
    "        sampled_indexes = np.random.choice(len(self.exp_prio_tuples), batch_size, p = sampling_probabilities)\n",
    "        sampled_tuples = [self.exp_prio_tuples[idx] for idx in sampled_indexes]\n",
    "        ep_idx = [tup[0] for tup in sampled_tuples]\n",
    "        #print(ep_idx)\n",
    "        exp_idx = [tup[1] for tup in sampled_tuples]\n",
    "        #print(exp_idx)\n",
    "        sampledTraces = []\n",
    "        if self.episode_index <= self.buffer_size:\n",
    "            idx_offset = 0\n",
    "        else:\n",
    "            idx_offset = self.episode_index - self.buffer_size\n",
    "        print('idx_offset = ' + str(idx_offset) + ', self.episode_index = ' + str(self.episode_index) + ', len(self.buffer) = ' + str(len(self.buffer)))\n",
    "        for i in range(0,batch_size):\n",
    "            sampled_ep = myBuffer.buffer[ep_idx[i] - idx_offset]#does This not work?\n",
    "            #print('len(sampled_ep) =' + str(len(sampled_ep)))\n",
    "            sampled_ep = np.reshape(np.array(sampled_ep),[len(sampled_ep),6])\n",
    "            sampled_trace = sampled_ep[exp_idx[i]+1-self.trace_length:exp_idx[i]+1]\n",
    "            sampledTraces.append(sampled_trace)\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        #print(sampledTraces.shape)\n",
    "        return np.reshape(sampledTraces,[batch_size*self.trace_length,6])\n",
    "    \n",
    "    def save(self, path2mdl):\n",
    "        #Save only last 40 experiences in buffer otherwise ridiculously large file\n",
    "        np.save(path2mdl + '/experienceBuffer.npy', self.buffer[-40:])\n",
    "    \n",
    "    def load(self, path2mdl):\n",
    "        self.buffer = list(np.load(path2mdl + '/experienceBuffer.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting the training parameters\n",
    "batch_size = 16 #How many experience traces to use for each training step.\n",
    "trace_length = 8 #How long each experience trace will be when training\n",
    "update_freq = 5 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "\n",
    "prioritized_replay = False\n",
    "load_model = False #Whether to load a saved model.\n",
    "if load_model == True:\n",
    "    last_saved_ep = 3000 #This parameter has to be updated to the last checkpoint\n",
    "else:\n",
    "    last_saved_ep = 0\n",
    "path2mdl = \"../DeepRL-Agents-Results/drqn\" #The path to save our model to.\n",
    "path2center = \"../DeepRL-Agents-Results/Center\" #The path to save the Center information to\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "buffer_size = 100 #Size of the episode buffer in number of episodes\n",
    "max_epLength = 300 #The max allowed length of our episode.\n",
    "anneling_steps = max_epLength*1000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = max_epLength*100 #max_epLength*100 #How many steps of random actions before training begins. need to be a multiple of max_epLength\n",
    "time_per_step =  0.025 #Length of each step used in gif creation\n",
    "summaryLength = 100 #Number of epidoes to periodically save for analysis\n",
    "tau = 0.001 #Rate at with the target network is update in regards to the main network\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████████████████████████████████████ | 82/83 [00:00<00:00, 384.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "\n",
    "#We define the cells for the primary and target q-networks\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main',learning_rate)\n",
    "targetQN = Qnetwork(h_size,cellT,'target',learning_rate)\n",
    "trainables = tf.trainable_variables()\n",
    "init = tf.global_variables_initializer()\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path2mdl):\n",
    "    os.makedirs(path2mdl)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path2mdl)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "        #Rough (over)estimate of the total number of steps since the beginning of training\n",
    "        total_steps = last_saved_ep*300/update_freq\n",
    "        myBuffer = experience_buffer(trace_length, buffer_size)\n",
    "        myBuffer.load(path2mdl)\n",
    "    else:\n",
    "        #INITIALIZE VARIABLES AND MODEL\n",
    "\n",
    "\n",
    "        myBuffer = experience_buffer(trace_length,buffer_size)\n",
    "\n",
    "        total_steps = 0\n",
    "        \n",
    "        sess.run(init)\n",
    "        #Write the first line of the master log-file for the Control Center\n",
    "        with open(path2center + '/log.csv', 'w') as myfile:\n",
    "            wr = csv.writer(myfile, quoting=csv.QUOTE_ALL, lineterminator = '\\n')\n",
    "            wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])   \n",
    "        #Set the target network to be equal to the primary network.\n",
    "        updateTarget(targetOps,sess)\n",
    "    \n",
    "    for i in range(last_saved_ep, num_episodes):\n",
    "        #print(i)\n",
    "        episodeBuffer = []\n",
    "        \n",
    "        #Reset environment and get first new observation\n",
    "        env.new_episode()\n",
    "        if use_other_buffers == True:\n",
    "            st = game.get_state()\n",
    "            dP = st.depth_buffer\n",
    "            lP = st.labels_buffer\n",
    "            sP = st.screen_buffer\n",
    "            s = processBuffers(image_size, dP, lP, sP)\n",
    "        else:\n",
    "            sP = env.get_state().screen_buffer\n",
    "            s = processImage(sP, image_size)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #Reset the recurrent layer's hidden state every episode\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size])) \n",
    "        #The Q-Network\n",
    "        while j < max_epLength:\n",
    "            \n",
    "            if image_chls == 2:\n",
    "                s_in = s[0:-image_size*image_size]\n",
    "            else:\n",
    "                s_in = s\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                #Only update the state of the RNN layer\n",
    "\n",
    "                state1 = sess.run(mainQN.rnn_state,\n",
    "                                  feed_dict={mainQN.scalarInput:[s_in/255.0],\n",
    "                                             mainQN.trainLength:1, \n",
    "                                             mainQN.state_in:state,\n",
    "                                             mainQN.batch_size:1})\n",
    "                #Choose an action randomly\n",
    "                a = np.random.randint(0,a_size)\n",
    "                \n",
    "            else:\n",
    "                #Update the state of the RNN layer AND choose the best action\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\n",
    "                                     feed_dict={mainQN.scalarInput:[s_in/255.0],\n",
    "                                                mainQN.trainLength:1,\n",
    "                                                mainQN.state_in:state,\n",
    "                                                mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "                \n",
    "            r = env.make_action(actions_list[a])\n",
    "            d = env.is_episode_finished()\n",
    "            if d == False:\n",
    "                if use_other_buffers == True:\n",
    "                    st1 = game.get_state()\n",
    "                    d1P = st1.depth_buffer\n",
    "                    l1P = st1.labels_buffer\n",
    "                    s1P = st1.screen_buffer\n",
    "                    s1 = processBuffers(image_size, d1P, l1P, s1P)\n",
    "                else:\n",
    "                    s1P = env.get_state().screen_buffer\n",
    "                    s1 = processImage(s1P, image_size)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            total_steps += 1\n",
    "            \n",
    "            #Compute the td error to use for prioritized replay\n",
    "            if image_chls == 2:\n",
    "                s1_in = s1[0:-image_size*image_size]\n",
    "            else:\n",
    "                s1_in = s1\n",
    "            \n",
    "            Q1 = sess.run(mainQN.predict,\n",
    "                          feed_dict={mainQN.scalarInput:[s1_in/255.0],\n",
    "                                     mainQN.trainLength:1,\n",
    "                                     mainQN.state_in:state1,\n",
    "                                     mainQN.batch_size:1})\n",
    "                    \n",
    "            Q2 = sess.run(targetQN.Qout,\n",
    "                          feed_dict={targetQN.scalarInput:[s1_in/255.0],\n",
    "                                     targetQN.trainLength:1,\n",
    "                                     targetQN.state_in:state1,\n",
    "                                     targetQN.batch_size:1})\n",
    "            \n",
    "            #print('Q1.shape = ' + str(Q1.shape))\n",
    "            #print('Q2.shape = ' + str(Q2.shape))        \n",
    "            end_multiplier = -(d - 1)\n",
    "            doubleQ = Q2[0, Q1]\n",
    "            #print('doubleQ.shape = ' + str(doubleQ.shape))\n",
    "            targetQ = r + (y*doubleQ * end_multiplier)\n",
    "            #print('targetQ.shape = ' + str(targetQ.shape))\n",
    "            currentaction = np.array(a, ndmin=1)\n",
    "            #print('currentaction.shape = ' + str(currentaction.shape))\n",
    "\n",
    "            td = sess.run(mainQN.td_error,\n",
    "                     feed_dict={mainQN.scalarInput:[s_in/255.0],\n",
    "                                mainQN.targetQ:targetQ,\n",
    "                                mainQN.actions:currentaction,\n",
    "                                mainQN.trainLength:1,\n",
    "                                mainQN.state_in:state,\n",
    "                                mainQN.batch_size:1})\n",
    "            \n",
    "\n",
    "            \n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d,td]),[1,6]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                    print('epsilon is = ' + str(e))\n",
    "                #Update the networks at a cetain frequency (every n experiences)\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    updateTarget(targetOps,sess)\n",
    "                    #Reset the recurrent layer's hidden state\n",
    "                    state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size])) \n",
    "                    #Get a random batch of experiences.\n",
    "                    if prioritized_replay == True:\n",
    "                        trainBatch = myBuffer.PRsample(batch_size)\n",
    "                    else:\n",
    "                        trainBatch = myBuffer.sample(batch_size)\n",
    "\n",
    "                    train_s = list(zip(trainBatch[:, 0]))\n",
    "                    train_s1 = list(zip(trainBatch[:, 3]))\n",
    "                    train_s = np.vstack(train_s)\n",
    "                    train_s1 = np.vstack(train_s1)\n",
    "\n",
    "                    if image_chls == 2:\n",
    "                        train_s = train_s[:,0:-image_size*image_size]\n",
    "                        train_s1 = train_s1[:,0:-image_size*image_size]\n",
    "\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,\n",
    "                                  feed_dict={mainQN.scalarInput:np.vstack(train_s1/255.0),\n",
    "                                             mainQN.trainLength:trace_length,\n",
    "                                             mainQN.state_in:state_train,\n",
    "                                             mainQN.batch_size:batch_size})\n",
    "                    \n",
    "                    Q2 = sess.run(targetQN.Qout,\n",
    "                                  feed_dict={targetQN.scalarInput:np.vstack(train_s1/255.0),\n",
    "                                             targetQN.trainLength:trace_length,\n",
    "                                             targetQN.state_in:state_train,\n",
    "                                             targetQN.batch_size:batch_size})\n",
    "                    \n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size*trace_length),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    \n",
    "                    #Update the network with our target values.\n",
    "                    sess.run(mainQN.updateModel,\n",
    "                             feed_dict={mainQN.scalarInput:np.vstack(train_s/255.0),\n",
    "                                        mainQN.targetQ:targetQ,\n",
    "                                        mainQN.actions:trainBatch[:,1],\n",
    "                                        mainQN.trainLength:trace_length,\n",
    "                                        mainQN.state_in:state_train,\n",
    "                                        mainQN.batch_size:batch_size})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            state = state1\n",
    "            \n",
    "            if use_other_buffers == True:\n",
    "                lP = l1P\n",
    "                dP = d1P\n",
    "                sP = s1P\n",
    "            else:\n",
    "                sP = s1P\n",
    "            \n",
    "\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        #Add the episode to the experience buffer\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        episodeBuffer = list(zip(bufferArray))\n",
    "        myBuffer.add(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0 and i != last_saved_ep:\n",
    "            saver.save(sess, path2mdl + '/model-'+str(i)+'.cptk', global_step = i)\n",
    "            myBuffer.save(path2mdl)\n",
    "            print (\"Saved Model\")\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            #print (total_steps,np.mean(rList[-summaryLength:]), e)\n",
    "            saveToCenter(i,rList,jList,\n",
    "                         np.reshape(np.array(episodeBuffer),[len(episodeBuffer),6]),\n",
    "                         summaryLength,\n",
    "                         h_size,sess,mainQN,time_per_step,\n",
    "                         image_size, image_chls, image_chls,\n",
    "                         path2center)\n",
    "    saver.save(sess,path2mdl + '/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.experience_buffer'>\n",
      "<class 'list'>\n",
      "100\n",
      "<class 'list'>\n",
      "100\n",
      "(51, 6)\n"
     ]
    }
   ],
   "source": [
    "print(type(myBuffer))\n",
    "print(type(myBuffer.buffer))\n",
    "print(len(myBuffer.buffer))\n",
    "#myBuffer.save(path2mdl)\n",
    "#myBuffer = experience_buffer()\n",
    "#myBuffer.load(path2mdl)\n",
    "print(type(myBuffer.buffer))\n",
    "print(len(myBuffer.buffer))\n",
    "print(myBuffer.buffer[98].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (270,1,1,6) (7056,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1fe6e624e7e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainBatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyBuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPRsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-8aa26dc0fe3e>\u001b[0m in \u001b[0;36mPRsample\u001b[1;34m(self, batch_size, trace_length, i, num_episodes)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mpriorities_poweralpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0msum_priorities_poweralpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpriorities_poweralpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0msampling_probabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpriorities_poweralpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum_priorities_poweralpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m#Sample episodes using the computed distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0msampled_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampling_probabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (270,1,1,6) (7056,) "
     ]
    }
   ],
   "source": [
    "trainBatch = myBuffer.PRsample(batch_size,trace_length, i, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00737692]\n",
      "(21, 6)\n",
      "(21,)\n",
      "(21,)\n",
      "(0, 9, 0.006785107310861349)\n",
      "0\n",
      "0.4997\n",
      "(14,)\n",
      "1.22897394469\n",
      "(14,)\n",
      "[7 2 1 8]\n",
      "[0, 0, 0, 0]\n",
      "[14, 9, 8, 15]\n",
      "(4, 8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(td)\n",
    "episode = episodeBuffer\n",
    "episode = np.reshape(np.array(episode),[len(episode),6])\n",
    "print(episode.shape)\n",
    "print(episode[:, 5].shape)\n",
    "td_error = episode[:, 5]\n",
    "priority = np.absolute(td_error) + 1e-9 #proportionnal priority\n",
    "print(priority.shape)\n",
    "#Append episode to the priority replay tuple list\n",
    "#Every experience in the episode has a tuple of the form:\n",
    "#(episode_index, experience_index, priority)\n",
    "episode_index = 0\n",
    "\n",
    "exp_prio_tuples = []\n",
    "for experience_index in range(trace_length-1, len(episode[:, 5])):\n",
    "    tup = (episode_index, experience_index, float(priority[experience_index]))\n",
    "    exp_prio_tuples.append(tup)\n",
    "\n",
    "\n",
    "print(exp_prio_tuples[2])\n",
    "print(exp_prio_tuples[0][0])\n",
    "\n",
    "priorities_poweralpha = np.power([tup[2] for tup in exp_prio_tuples],myBuffer.alpha)\n",
    "print(myBuffer.alpha)\n",
    "print(priorities_poweralpha.shape)\n",
    "sum_priorities_poweralpha = np.sum(priorities_poweralpha)\n",
    "print(sum_priorities_poweralpha)\n",
    "sampling_probabilities = np.divide(priorities_poweralpha, sum_priorities_poweralpha)\n",
    "print(sampling_probabilities.shape)\n",
    "sampled_indexes = np.random.choice(len(exp_prio_tuples), batch_size, p = sampling_probabilities)\n",
    "print(sampled_indexes)\n",
    "\n",
    "sampled_tuples = [exp_prio_tuples[idx] for idx in sampled_indexes]\n",
    "ep_idx = [tup[0] for tup in sampled_tuples]\n",
    "print(ep_idx)\n",
    "exp_idx = [tup[1] for tup in sampled_tuples]\n",
    "print(exp_idx)\n",
    "sampledTraces = []\n",
    "for i in range(0,batch_size):\n",
    "    sampled_ep = myBuffer.buffer[ep_idx[i]]\n",
    "    sampled_ep = np.reshape(np.array(sampled_ep),[len(sampled_ep),6])\n",
    "    sampled_exp = sampled_ep[exp_idx[i]-(trace_length):exp_idx[i]] \n",
    "    sampledTraces.append(sampled_exp)\n",
    "sampledTraces = np.array(sampledTraces)\n",
    "print(sampledTraces.shape)\n",
    "result = np.reshape(sampledTraces,[batch_size*trace_length,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 6)\n",
      "0\n",
      "14\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(sampledTraces[3].shape)\n",
    "print(exp_prio_tuples[0][0])\n",
    "print(len(exp_prio_tuples))\n",
    "exp_prio_tuples.pop(0)\n",
    "print(len(exp_prio_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
